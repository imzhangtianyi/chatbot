{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_t = pd.read_csv('QandA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/anaconda3/envs/insight/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# Tokenize chats\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "txt = df_t.QandA.apply(lambda s: tokenizer.tokenize(s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stem_words(l):\n",
    "    return [porter_stemmer.stem(words) for words in l]\n",
    "\n",
    "txt_list = list(map(stem_words,txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(l):\n",
    "    return [wordnet_lemmatizer.lemmatize(words) for words in l]\n",
    "\n",
    "txt_list = list(map(lemmatize_words,txt_list))\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['hello', 'hi', 'welcom', 'headout', 'know', 'experi', 'refer', 'help', 'ani', 'chat', 'problem', 'may', 'reach', 'need', 'let', 'u', 'feel', 'free', 'contact', 'realli', 'appreci', 'could', 'rate', 'chat', 'thank', 'today', 'wa', 'nice', 'talk', 'great', 'day', 'goodby', 'would', 'like','plea', 'wait', 'minut', 'check', 'thi', 'anyth', 'el', 'step', 'away', 'assist', 'custom', 'bye', 'hey', 'ok','get', 'ye', 'safari', 'khalifa', 'burj', 'aquarium', 'roman', 'palatin', 'vatican', 'dubai'])\n",
    "def remove_stopwords(l):\n",
    "    return [word for word in l if word not in stop_words]\n",
    "txt_list = list(map(remove_stopwords,txt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l = pd.Series(list(map(lambda x: ' '.join(x),txt_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66180,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(txt_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = txt_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "from corextopic import corextopic as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english', max_features=20000, binary=True)\n",
    "c = cv.fit_transform(df_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66180, 10330)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(np.asarray(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor\n",
    "anchor_words = [['cashback', 'cash'], \n",
    "                ['refund', 'cancel'], \n",
    "                ['child', 'adult','year','old','age','kid'], \n",
    "                ['seat', 'choos','select','section','offic','exact','togeth'],\n",
    "                ['discount','coupon','code','use'],\n",
    "                ['card','payment','work','complet','error','issu','differ','tri']\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 3.34 s, total: 1min 43s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anchored_topic_model = ct.Corex(n_hidden=20, max_iter=200, seed=1)\n",
    "anchored_topic_model.fit(c, words=words, anchors=anchor_words, anchor_strength=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: cashback,cash,wallet,facebook,log,creat,account,user,virtual,futur\n",
      "1: cancel,refund,polici,0100,347,897,strict,reschedul,touch,amend\n",
      "2: adult,child,year,old,age,kid,abov,yr,18,price\n",
      "3: seat,select,choos,offic,section,exact,togeth,box,best,intellig\n",
      "4: use,discount,code,coupon,promo,browser,app,wow,alreadi,love\n",
      "5: tri,card,differ,work,issu,payment,complet,error,credit,chrome\n",
      "6: ticket,buy,purchas,book,onlin,want,onli,valid,befor,look\n",
      "7: park,bird,king,dolphin,lion,garden,jurong,bay,seal,ice\n",
      "8: http,www,com,tour,chapel,sistin,unit,arab,world,museum\n",
      "9: tower,eiffel,cruis,pari,sein,summit,floor,franc,bateaux,river\n",
      "10: hotel,pick,drop,desert,dinner,transfer,pickup,citi,dune,airport\n",
      "11: pm,time,slot,30,date,open,septemb,00,hour,2018\n",
      "12: group,servic,peopl,provid,altern,small,ask,resid,question,flag\n",
      "13: support,write,request,apolog,delay,inventori,connect,respons,fulfil,manag\n",
      "14: sorri,avail,unfortun,abl,websit,vendor,inconveni,sold,possibl,sinc\n",
      "15: email,receiv,reserv,id,sent,make,send,confirm,team,number\n",
      "16: theater,york,new,state,host,theatr,outsid,odd,empir,school\n",
      "17: line,skip,guid,access,audio,entri,entranc,audioguid,doe,escort\n",
      "18: ride,palac,versail,ski,water,fountain,notr,dame,polar,aquaventur\n",
      "19: forum,st,peter,la,hill,colosseum,en,itali,basilica,por\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = anchored_topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 3.84 s, total: 1min 48s\n",
      "Wall time: 55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topic_model = ct.Corex(n_hidden=20, words=words, max_iter=200, verbose=False, seed=1)\n",
    "topic_model.fit(c, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: la,por,en,una,que,entrada,para,lo,si,se\n",
      "1: http,www,skip,tour,com,line,guid,tower,eiffel,colosseum\n",
      "2: card,use,credit,code,tri,facebook,coupon,payment,log,discount\n",
      "3: cruis,arab,emir,unit,dinner,desert,dhow,river,sein,dune\n",
      "4: pm,time,30,slot,00,hour,open,10,septemb,2018\n",
      "5: empir,build,speak,english,state,piazza,spanish,translat,ancient,languag\n",
      "6: seat,section,exact,best,togeth,choos,intellig,orchestra,map,avail\n",
      "7: chapel,sistin,world,museum,pas,univers,studio,singapor,ferrari,sentosa\n",
      "8: email,receiv,sent,send,id,confirm,reserv,number,familia,mail\n",
      "9: hotel,pick,drop,point,transfer,pickup,citi,meet,locat,bu\n",
      "10: child,year,adult,old,age,kid,abov,yr,18,daughter\n",
      "11: box,offic,request,paid,chrome,write,googl,support,error,complet\n",
      "12: cancel,inconveni,refund,polici,sorri,delay,apolog,strict,ha,48\n",
      "13: book,make,websit,onli,purchas,abl,onlin,becaus,say,befor\n",
      "14: trip,round,depart,boat,board,assur,question,wheelchair,inquiri,vegetarian\n",
      "15: 19,max,22nd,rang,195,play,shot,fir,reduc,46\n",
      "16: june,tuesday,friday,aug,fountain,saturday,monday,sold,7th,15th\n",
      "17: new,york,understand,case,local,partner,told,someon,ask,busi\n",
      "18: park,price,offer,garden,want,group,famili,plan,peopl,cost\n",
      "19: ticket,buy,date,select,look,sell,phantom,3rd,opera,28\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<corextopic.corextopic.Corex at 0x7fb21be96128>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a matrix where rows are samples (docs) and columns are features (words)\n",
    "X = np.array([[0,0,0,1,1],\n",
    "              [1,1,1,0,0],\n",
    "              [1,1,1,1,1]], dtype=int)\n",
    "# Sparse matrices are also supported\n",
    "X = ss.csr_matrix(X)\n",
    "# Word labels for each column can be provided to the model\n",
    "words = ['dog', 'cat', 'fish', 'apple', 'orange']\n",
    "# Document labels for each row can be provided\n",
    "docs = ['fruit doc', 'animal doc', 'mixed doc']\n",
    "\n",
    "# Train the CorEx topic model\n",
    "topic_model = ct.Corex(n_hidden=2)  # Define the number of latent (hidden) topics to use.\n",
    "topic_model.fit(X, words=words, docs=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: dog,cat,fish\n",
      "2: apple,orange\n"
     ]
    }
   ],
   "source": [
    "topics = topic_model.get_topics()\n",
    "for topic_n,topic in enumerate(topics):\n",
    "    words,mis = zip(*topic)\n",
    "    topic_str = str(topic_n+1)+': '+','.join(words)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: animal doc,mixed doc,fruit doc\n",
      "2: animal doc,mixed doc,fruit doc\n"
     ]
    }
   ],
   "source": [
    "top_docs = topic_model.get_top_docs()\n",
    "for topic_n, topic_docs in enumerate(top_docs):\n",
    "    docs,probs = zip(*topic_docs)\n",
    "    topic_str = str(topic_n+1)+': '+','.join(docs)\n",
    "    print(topic_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
